{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683714c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "import pandas as pd\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400612b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42b987",
   "metadata": {},
   "source": [
    "## functions to crop the card from the image and correct the orientation of the card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d66cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_threshold = 0.001\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def get_distances(points):\n",
    "    card_width = max(np.linalg.norm(points[0] - points[1]),np.linalg.norm(points[3] - points[2]))\n",
    "    card_height = max(np.linalg.norm(points[1] - points[2]),np.linalg.norm(points[0] - points[3]))\n",
    "    \n",
    "    return card_width,card_height\n",
    "\n",
    "\n",
    "\n",
    "def camscanner(points,shape_wh,img):\n",
    "    card_width,card_height = shape_wh\n",
    "    pts1 = np.float32(points[[0,1,3]])\n",
    "    pts2 = np.float32([[0,0],[card_width,0],[0,card_height]])#[card_width,card_height]])\n",
    "    matrix = cv2.getAffineTransform(pts1,pts2)\n",
    "    result = cv2.warpAffine(img, matrix, (int(card_width), int(card_height)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa509eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class params:\n",
    "    def __init__(self) -> None:\n",
    "        self.image_folder = '../trainable_dataset_files/test.part'\n",
    "        self.output_folder = '../output/'\n",
    "        self.plot_flag = True\n",
    "        self.txt_out = True\n",
    "        self.cfg = '../cfg/yolov3.cfg'\n",
    "        self.weights_path = '../weights/latest.pt'\n",
    "        self.conf_thres = 0.1\n",
    "        self.nms_thres = 0.0\n",
    "        self.img_size = 32 * 19\n",
    "        self.class_path = '../data/icdar.names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "095a333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cdc8dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fields =  {\n",
    "    \"file_name\":\"\",\n",
    "    \"num_card\" : \"\",\n",
    "    \"P1_x\": \"\",\n",
    "    \"P1_y\": \"\",\n",
    "    \"P2_x\": \"\",\n",
    "    \"P2_y\": \"\",\n",
    "    \"P3_x\": \"\",\n",
    "    \"P3_y\": \"\",\n",
    "    \"P4_x\": \"\",\n",
    "    \"P4_y\": \"\",\n",
    "    \"cls_pred\": \"\",\n",
    "    \"conf\": \"\",\n",
    "    \"cls_conf\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d99c3",
   "metadata": {},
   "source": [
    "## generate the output_fields.csv, so that it will have all the fields that can be used for evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef624b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_out_filename = \"output_fields.csv\"\n",
    "if opt.txt_out:\n",
    "    df = pd.DataFrame(columns=out_fields.keys())\n",
    "    df.to_csv(txt_out_filename,mode=\"w\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062e8fd",
   "metadata": {},
   "source": [
    "## plot the co-ordinates on the card and save the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system('rm -rf ' + opt.output_folder)\n",
    "os.makedirs(opt.output_folder, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "model = Darknet(opt.cfg, opt.img_size)\n",
    "\n",
    "weights_path = opt.weights_path\n",
    "if weights_path.endswith('.weights'):  # saved in darknet format\n",
    "    load_weights(model, weights_path)\n",
    "else:  # endswith('.pt'), saved in pytorch format\n",
    "    checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    del checkpoint\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# Set Dataloader\n",
    "classes = load_classes(opt.class_path)  # Extracts class labels from file\n",
    "dataloader = load_images_test(opt.image_folder, batch_size=1, img_size=opt.img_size)\n",
    "print(classes)\n",
    "\n",
    "# Bounding-box colors\n",
    "color_list = [[random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)] for _ in range(len(classes))]\n",
    "\n",
    "\n",
    "#   imgs = []  # Stores image paths\n",
    "#   img_detections = []  # Stores detections for each image index\n",
    "prev_time = time.time()\n",
    "\n",
    "for batch_i, (img_paths, img,area) in enumerate(dataloader):\n",
    "    \n",
    "    if img_paths[0] != \"../midv_dataset/02_aut_drvlic_new/images/HA/HA02_20.tif\":\n",
    "        continue\n",
    "    print(batch_i, img.shape)\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        chip = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "        pred = model(chip)\n",
    "        print(np.array(pred[:, :, 8].cpu().numpy()))\n",
    "        print(pred[:, :, 8].shape,pred.shape)\n",
    "        pred = pred[pred[:, :, 8] > opt.conf_thres]\n",
    "        detections = [None]\n",
    "        if len(pred) > 0:\n",
    "            detections = non_max_suppression_test(pred.unsqueeze(0),area,area_threshold, opt.conf_thres,opt.nms_thres)\n",
    "#               img_detections.extend(detections)\n",
    "#               imgs.extend(img_paths)\n",
    "\n",
    "    print('Batch %d... (Done %.3f s)' % (batch_i, time.time() - prev_time))\n",
    "    prev_time = time.time()\n",
    "\n",
    "    if len(detections) <1 :\n",
    "        out_fields_copy = copy(out_fields)\n",
    "        out_fields_copy[\"file_name\"] = path\n",
    "        out_fields_copy[\"num_card\"] = 0\n",
    "        out_row = pd.DataFrame(out_fields_copy,index = [0])\n",
    "        out_row.to_csv(txt_out_filename, mode = \"a\", index = False, header = False)\n",
    "        continue\n",
    "\n",
    "    if len(detections) == 1 and detections[0] is None:\n",
    "        continue\n",
    "\n",
    "    # Iterate through images and save plot of detections\n",
    "    # for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "    path = img_paths[0]\n",
    "    detections = detections[0]\n",
    "#       print(detections)\n",
    "    print(\"image %g: '%s'\" % (batch_i,path))\n",
    "    cropped_ploted = False\n",
    "    results_img_path = os.path.join(opt.output_folder, path.split('/')[-1])\n",
    "    cropped_img_path = os.path.join(opt.output_folder, \"cropped_\"+path.split('/')[-1])\n",
    "\n",
    "    img = np.uint8(img.transpose(1,2,0) * 255)\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (opt.img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (opt.img_size / max(img.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = opt.img_size - pad_y\n",
    "    unpad_w = opt.img_size - pad_x\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        unique_classes = detections[:, -1].cpu().unique()\n",
    "        bbox_colors = random.sample(color_list, len(unique_classes))\n",
    "\n",
    "        results_txt_path = results_img_path + '.txt'\n",
    "        if os.path.isfile(results_txt_path):\n",
    "            os.remove(results_txt_path)\n",
    "\n",
    "        for i in unique_classes:\n",
    "            n = (detections[:, -1].cpu() == i).sum()\n",
    "            print('%g %ss' % (n, classes[int(i)]))\n",
    "\n",
    "        for num_card, (P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y, conf, cls_conf, cls_pred) in enumerate(detections):\n",
    "            P1_y = max((((P1_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
    "            P1_x = max((((P1_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
    "            P2_y = max((((P2_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
    "            P2_x = max((((P2_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
    "            P3_y = max((((P3_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
    "            P3_x = max((((P3_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
    "            P4_y = max((((P4_y - pad_y // 2) / unpad_h) * img.shape[0]).round().item(), 0)\n",
    "            P4_x = max((((P4_x - pad_x // 2) / unpad_w) * img.shape[1]).round().item(), 0)\n",
    "            \n",
    "            # write to file\n",
    "            if opt.txt_out:\n",
    "                with open(results_txt_path, 'a') as file:\n",
    "                    file.write(('%g %g %g %g %g %g %g %g %g %g \\n') % (P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y, cls_pred, cls_conf * conf))\n",
    "                    \n",
    "                out_fields_copy = copy(out_fields)\n",
    "                out_fields_copy[\"file_name\"] = path\n",
    "                out_fields_copy[\"num_card\"] = num_card+1\n",
    "                out_fields_copy[\"P1_x\"] = P1_x\n",
    "                out_fields_copy[\"P1_y\"] = P1_y\n",
    "                out_fields_copy[\"P2_x\"] = P2_x\n",
    "                out_fields_copy[\"P2_y\"] = P2_y\n",
    "                out_fields_copy[\"P3_x\"] = P3_x\n",
    "                out_fields_copy[\"P3_y\"] = P3_y\n",
    "                out_fields_copy[\"P4_x\"] = P4_x\n",
    "                out_fields_copy[\"P4_y\"] = P4_y\n",
    "                out_fields_copy[\"cls_pred\"] = cls_pred.int().item()\n",
    "                out_fields_copy[\"conf\"] = conf.float().item()\n",
    "                out_fields_copy[\"cls_conf\"] = cls_conf.float().item()\n",
    "                out_row = pd.DataFrame(out_fields_copy,index = [0])\n",
    "                out_row.to_csv(txt_out_filename, mode = \"a\", index = False, header = False)\n",
    "                \n",
    "                \n",
    "\n",
    "            if opt.plot_flag:\n",
    "                # Add the bbox to the plot\n",
    "                label = '%s %.2f' % (classes[int(cls_pred)], conf)\n",
    "                color = bbox_colors[int(np.where(unique_classes == int(cls_pred))[0])]\n",
    "                plot_one_box([P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y], img, label=None, color=color,conf = conf)\n",
    "                poly_arr = np.array([P1_x, P1_y, P2_x, P2_y, P3_x, P3_y, P4_x, P4_y]).reshape(4,2)\n",
    "#                   card_width,card_height = get_distances(poly_arr)\n",
    "#                   result = camscanner(poly_arr,(card_width,card_height),img)\n",
    "#                   cropped_ploted = True\n",
    "        # cv2.imshow(path.split('/')[-1], img)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "#       if opt.plot_flag:\n",
    "    print(\"saving_file\")\n",
    "#       img = np.uint8(img.transpose(1,2,0))\n",
    "    # Save generated image with detections\n",
    "\n",
    "    print(img.shape,results_img_path.replace('.bmp', '.jpg').replace('.tif', '.jpg'),\"\\n\",\n",
    "              \"IS FILE SAVED:- \",cv2.imwrite(results_img_path.replace('.bmp', '.jpg').replace('.tif', '.jpg'), img[:,:,::-1]))\n",
    "#       if cropped_ploted:\n",
    "#           print(cv2.imwrite(cropped_img_path.replace('.bmp', '.jpg').replace('.tif', '.jpg'), result))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "detect(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0f970",
   "metadata": {},
   "source": [
    "## Model evaliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77402015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae1e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./output_fields.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be73230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>num_card</th>\n",
       "      <th>P1_x</th>\n",
       "      <th>P1_y</th>\n",
       "      <th>P2_x</th>\n",
       "      <th>P2_y</th>\n",
       "      <th>P3_x</th>\n",
       "      <th>P3_y</th>\n",
       "      <th>P4_x</th>\n",
       "      <th>P4_y</th>\n",
       "      <th>cls_pred</th>\n",
       "      <th>conf</th>\n",
       "      <th>cls_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./midv_dataset/29_irn_drvlic/images/HA/HA29_05...</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.414891</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./midv_dataset/01_alb_id/images/KA/KA01_12.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.692086</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./midv_dataset/47_usa_bordercrossing/images/KA...</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.901315</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./midv_dataset/41_srb_passport/images/KS/KS41_...</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.791177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./midv_dataset/14_deu_id_new/images/CS/CS14_15...</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930499</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>./midv_dataset/29_irn_drvlic/images/KS/KS29_22...</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302790</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>./midv_dataset/31_jpn_drvlic/images/HA/HA31_18...</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136218</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>./midv_dataset/45_ukr_passport/images/KS/KS45_...</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297593</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>./midv_dataset/03_aut_id_old/images/CA/CA03_18...</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213717</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>./midv_dataset/18_dza_passport/images/TS/TS18_...</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1435 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_name  num_card  P1_x  \\\n",
       "0     ./midv_dataset/29_irn_drvlic/images/HA/HA29_05...         1  48.0   \n",
       "1        ./midv_dataset/01_alb_id/images/KA/KA01_12.tif         1  33.0   \n",
       "2     ./midv_dataset/47_usa_bordercrossing/images/KA...         1  66.0   \n",
       "3     ./midv_dataset/41_srb_passport/images/KS/KS41_...         1  43.0   \n",
       "4     ./midv_dataset/14_deu_id_new/images/CS/CS14_15...         1  60.0   \n",
       "...                                                 ...       ...   ...   \n",
       "1430  ./midv_dataset/29_irn_drvlic/images/KS/KS29_22...         1  64.0   \n",
       "1431  ./midv_dataset/31_jpn_drvlic/images/HA/HA31_18...         1  71.0   \n",
       "1432  ./midv_dataset/45_ukr_passport/images/KS/KS45_...         1  25.0   \n",
       "1433  ./midv_dataset/03_aut_id_old/images/CA/CA03_18...         1  37.0   \n",
       "1434  ./midv_dataset/18_dza_passport/images/TS/TS18_...         1  14.0   \n",
       "\n",
       "       P1_y   P2_x   P2_y   P3_x   P3_y   P4_x   P4_y  cls_pred      conf  \\\n",
       "0     248.0  512.0  183.0  582.0  360.0  103.0  419.0         0  0.414891   \n",
       "1     218.0  524.0  183.0  560.0  354.0   68.0  392.0         0  0.692086   \n",
       "2     311.0  526.0  290.0  552.0  441.0   79.0  467.0         0  0.901315   \n",
       "3     182.0  568.0  187.0  557.0  391.0   24.0  389.0         0  0.791177   \n",
       "4     270.0  496.0  273.0  516.0  431.0   56.0  435.0         0  0.930499   \n",
       "...     ...    ...    ...    ...    ...    ...    ...       ...       ...   \n",
       "1430  215.0  566.0  223.0  561.0  415.0   48.0  400.0         0  0.302790   \n",
       "1431  299.0  459.0  269.0  462.0  391.0  116.0  404.0         0  0.136218   \n",
       "1432  182.0  563.0  165.0  563.0  406.0   32.0  411.0         0  0.297593   \n",
       "1433  286.0  394.0  192.0  493.0  315.0  135.0  408.0         0  0.213717   \n",
       "1434  168.0  575.0  163.0  584.0  390.0    4.0  392.0         0  0.175390   \n",
       "\n",
       "      cls_conf  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          1.0  \n",
       "...        ...  \n",
       "1430       1.0  \n",
       "1431       1.0  \n",
       "1432       1.0  \n",
       "1433       1.0  \n",
       "1434       1.0  \n",
       "\n",
       "[1435 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5beeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from shapely.geometry import Polygon\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all IOU values along with their corresponding file paths\n",
    "all_iou = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    # Construct the file path by appending a dot (current directory) to the 'file_name'\n",
    "    each_filepath = \".\" + row.file_name\n",
    "\n",
    "    # Check if the file path matches a certain pattern using regex\n",
    "    if len(re.findall(r\"\\/\\w{2}\\/\\w{2}\\d{2}\\_\\d{2}\\.tif$\", each_filepath)) < 1:\n",
    "        # If the pattern does not match, skip to the next iteration\n",
    "        continue\n",
    "\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(each_filepath)\n",
    "    img_h, img_w = img.shape[:2]\n",
    "\n",
    "    # Construct the JSON file path by replacing '.tif' with '.json' in 'each_filepath'\n",
    "    json_path = re.sub(r\"\\.tif$\", \".json\", each_filepath)\n",
    "    \n",
    "    # Modify the 'json_path' to replace '/images/' with '/ground_truth/'\n",
    "    json_path = re.sub(\"\\/images\\/\", '/ground_truth/', json_path, re.DOTALL)\n",
    "\n",
    "    # Create a Polygon object representing the predicted region\n",
    "    pred_poly = Polygon([\n",
    "        [(row.P1_x / 608) * img_w, (row.P1_y / 608) * img_h],\n",
    "        [(row.P2_x / 608) * img_w, (row.P2_y / 608) * img_h],\n",
    "        [(row.P3_x / 608) * img_w, (row.P3_y / 608) * img_h],\n",
    "        [(row.P4_x / 608) * img_w, (row.P4_y / 608) * img_h],\n",
    "    ])\n",
    "\n",
    "    # Read the ground truth points from the JSON file\n",
    "    with open(json_path, \"r\") as f:\n",
    "        quad_json = json.load(f)\n",
    "        quad_points = np.array(quad_json[\"quad\"])\n",
    "\n",
    "    # Create a Polygon object representing the ground truth region\n",
    "    ground_truth_poly = Polygon(quad_points)\n",
    "\n",
    "    # Calculate intersection and union areas to compute Intersection over Union (IOU)\n",
    "    inter_area = ground_truth_poly.intersection(pred_poly).area\n",
    "    union_area = ground_truth_poly.union(pred_poly).area\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    # Append the file path and IOU value to the list\n",
    "    all_iou.append([each_filepath, iou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4790faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../midv_dataset/29_irn_drvlic/images/HA/HA29_0...</td>\n",
       "      <td>0.929793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../midv_dataset/01_alb_id/images/KA/KA01_12.tif</td>\n",
       "      <td>0.967739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../midv_dataset/47_usa_bordercrossing/images/K...</td>\n",
       "      <td>0.931991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../midv_dataset/41_srb_passport/images/KS/KS41...</td>\n",
       "      <td>0.947237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../midv_dataset/14_deu_id_new/images/CS/CS14_1...</td>\n",
       "      <td>0.945262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name       iou\n",
       "0  ../midv_dataset/29_irn_drvlic/images/HA/HA29_0...  0.929793\n",
       "1    ../midv_dataset/01_alb_id/images/KA/KA01_12.tif  0.967739\n",
       "2  ../midv_dataset/47_usa_bordercrossing/images/K...  0.931991\n",
       "3  ../midv_dataset/41_srb_passport/images/KS/KS41...  0.947237\n",
       "4  ../midv_dataset/14_deu_id_new/images/CS/CS14_1...  0.945262"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_df = pd.DataFrame(all_iou, columns=[\"file_name\", \"iou\"])\n",
    "iou_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4b585",
   "metadata": {},
   "source": [
    "## average IOU on all the testing set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54a9a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9478585441640888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_df.iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777c6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trainable_dataset_files/test.part\", \"r\") as f:\n",
    "    test_set = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ea35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_images_in_test_set  = len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809ca93",
   "metadata": {},
   "source": [
    "## detection accuracy, as if object was detected, it was added in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787424a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768550034036759"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] / num_of_images_in_test_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
